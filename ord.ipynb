{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3f8bfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"zouharvi/bio-mqm-dataset\", split='test')\n",
    "ds = ds.filter(lambda example: example['lang_src'] == 'en' and example['lang_tgt'] == 'zh').select_columns(['src', 'tgt']).rename_columns({'src': 'english', 'tgt': 'chinese'})\n",
    "ds_valid = datasets.load_dataset(\"zouharvi/bio-mqm-dataset\", split='validation')\n",
    "ds_valid = ds_valid.filter(lambda example: example['lang_src'] == 'en' and example['lang_tgt'] == 'zh').select_columns(['src', 'tgt']).rename_columns({'src': 'english', 'tgt': 'chinese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b813961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 4000\n",
      "验证集大小: 2000\n",
      "测试集大小: 1819\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    " \n",
    "max_dataset_size = 6000\n",
    "train_set_size = 4000\n",
    "valid_set_size = 2000\n",
    " \n",
    "class TRANS(Dataset):\n",
    "    def __init__(self, data_file:str):\n",
    "        self.data = self.load_data(data_file)\n",
    "    def __init__(self, data: datasets.Dataset):\n",
    "        self.data = {}\n",
    "        for idx in range(len(data)):\n",
    "            if idx >= max_dataset_size:\n",
    "                break\n",
    "            sample = data[idx]\n",
    "            self.data[idx] = {\n",
    "                'english': sample['english'],\n",
    "                'chinese': sample['chinese']\n",
    "            }\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx >= max_dataset_size:\n",
    "                    break\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "data = TRANS(ds)\n",
    "test_data = TRANS(ds_valid)\n",
    "train_data, valid_data = random_split(data, [train_set_size, valid_set_size])\n",
    "\n",
    "# data = TRANS('data/translation2019zh_train.json')\n",
    "# train_data, valid_data = random_split(data, [train_set_size, valid_set_size])\n",
    "# test_data = TRANS('data/translation2019zh_valid.json')\n",
    "print(\"训练集大小:\", len(train_data))\n",
    "print(\"验证集大小:\", len(valid_data))\n",
    "print(\"测试集大小:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e72c0785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    " \n",
    "#这是hugging face里的模型，需要科学上网\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    " \n",
    "#这是modelscope里的，国内可以直接访问\n",
    "# model_checkpoint =\"moxying/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    " \n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    " \n",
    "#每次给模型输入4批数据\n",
    "inputs = [train_data[s_idx][\"english\"] for s_idx in range(4)]\n",
    "targets = [train_data[s_idx][\"chinese\"] for s_idx in range(4)]\n",
    " \n",
    "model_inputs = tokenizer(\n",
    "    inputs, \n",
    "    padding=True, \n",
    "    max_length=max_input_length, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "  #默认情况下分词器会采用源语言的设定来编码文本，要编码目标语言则需要通过上下文管理器\n",
    "#  as_target_tokenizer(),否则中文分词器可能无法识别大部分的英文单词\n",
    "with tokenizer.as_target_tokenizer(): \n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        padding=True, \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    " \n",
    "end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "for idx, end_idx in enumerate(end_token_index):\n",
    "    labels[idx][end_idx+1:] = -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "017872ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    " \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    " \n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['english'])\n",
    "        batch_targets.append(sample['chinese'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs, \n",
    "        padding=True, \n",
    "        max_length=max_input_length,\n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets, \n",
    "            padding=True, \n",
    "            max_length=max_target_length,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    " \n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=32, shuffle=False, collate_fn=collote_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "970e6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    " \n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    " \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b61ed43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "import numpy as np\n",
    "\n",
    "bleu = BLEU()\n",
    " \n",
    "def test_loop(dataloader, model):\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            ).cpu().numpy()\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    " \n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    bleu_score = bleu.corpus_score(preds, labels).score\n",
    "    print(f\"BLEU: {bleu_score:>0.2f}\\n\")\n",
    "    return bleu_score\n",
    "\n",
    "# test_loop(valid_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040af81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe662818bad4cfbb5f2529ac0d8f832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f15324b6c8443d932ddc9247a77789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 11.99\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6a879fa2874c19a63c6ccadcd4a923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f0bb8658f744e5ad7b7907382a7fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 11.99\n",
      "\n",
      "Epoch 3/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c970250068c4e9596bb9369b70aec87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f2e0974d5645ad919f0dd62f4d7413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 11.99\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 3\n",
    " \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_bleu = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_bleu = test_loop(valid_dataloader, model)\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'ord_model/epoch_{t+1}_valid_bleu_{valid_bleu:0.2f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bebe4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'The most feared of these complications is endophthalmitis, a severe infection of the eye with extremely poor visual outcomes.', 'chinese': '这些并发症中最令人担心的是眼内炎，这是一种严重的眼睛感染，视力极差。'}\n",
      "最担心的这些并发症是内眼炎,眼部严重感染,视觉效果极差。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# model.load_state_dict(torch.load('epoch_1_valid_bleu_82.97_model_weights.bin'))\n",
    "\n",
    "def clean_translation(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # 去除多余的空格\n",
    "    text = re.sub(r\"\\s([?.!\\\"'])\", r\"\\1\", text)  # 去除句末的多余空格\n",
    "    return text\n",
    "\n",
    "def translate(text, tokenizer, model):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model.generate(inputs, max_length=5000, num_beams=8, early_stopping=True)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_translation(translated_text)\n",
    "\n",
    "# print(translate(\"However, these drugs are administered via intravitreal injections that are associated with sight-threatening complications.\", tokenizer, model))\n",
    "print(test_data[1])\n",
    "print(translate(test_data[1]['english'], tokenizer, model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
