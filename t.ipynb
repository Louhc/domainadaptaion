{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b813961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 300000\n",
      "验证集大小: 30000\n",
      "测试集大小: 39323\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    " \n",
    "max_dataset_size = 330000\n",
    "train_set_size = 300000\n",
    "valid_set_size = 30000\n",
    " \n",
    "class TRANS(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx >= max_dataset_size:\n",
    "                    break\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "data = TRANS('data/translation2019zh_train.json')\n",
    "train_data, valid_data = random_split(data, [train_set_size, valid_set_size])\n",
    "test_data = TRANS('data/translation2019zh_valid.json')\n",
    "print(\"训练集大小:\", len(train_data))\n",
    "print(\"验证集大小:\", len(valid_data))\n",
    "print(\"测试集大小:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133a3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af80bc7c7f4b4c048b6a7cf39d3eb8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-zh-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bf422f6d234ec4b333d4a9eeee57b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdc28b667c14e13be26897484823810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958e970151ab406f9b98863e339aad4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/807k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6891ea5897bb49509e2dee7e41741d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    " \n",
    "inputs = [train_data[s_idx][\"chinese\"] for s_idx in range(4)]\n",
    "targets = [train_data[s_idx][\"english\"] for s_idx in range(4)]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    inputs, \n",
    "    padding=True, \n",
    "    max_length=max_input_length, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with tokenizer.as_target_tokenizer(): \n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        padding=True, \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    " \n",
    "end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "for idx, end_idx in enumerate(end_token_index):\n",
    "    labels[idx][end_idx+1:] = -50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
