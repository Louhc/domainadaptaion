{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2960ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f913272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Number of training samples: 10000\n",
      "Number of validation samples: 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path, max_lines=float(\"inf\")):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        n = 0\n",
    "        for line in f:\n",
    "            n += 1\n",
    "            if n > max_lines: break\n",
    "\n",
    "            try:\n",
    "                item = json.loads(line.strip())\n",
    "                data.append(item)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONè§£ç é”™è¯¯ï¼š{e}ï¼Œè·³è¿‡è¯¥è¡Œ\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "train_data = load_data('translation2019zh_train.json', 10000)\n",
    "valid_data = load_data('translation2019zh_valid.json', 100)\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a65693c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒæ•°æ®æ¸…æ´—å®Œæ¯•ï¼Œå‰©ä½™ 10000 æ¡ã€‚\n",
      "éªŒè¯æ•°æ®æ¸…æ´—å®Œæ¯•ï¼Œå‰©ä½™ 100 æ¡ã€‚\n"
     ]
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    seen = set()\n",
    "    for item in data:\n",
    "        en = item['english'].strip()\n",
    "        zh = item['chinese'].strip()\n",
    "        if (en, zh) not in seen:\n",
    "            cleaned_data.append({'english': en, 'chinese': zh})\n",
    "            seen.add((en, zh))\n",
    "    return cleaned_data\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "valid_data = clean_data(valid_data)\n",
    "\n",
    "print(f\"è®­ç»ƒæ•°æ®æ¸…æ´—å®Œæ¯•ï¼Œå‰©ä½™ {len(train_data)} æ¡ã€‚\")\n",
    "print(f\"éªŒè¯æ•°æ®æ¸…æ´—å®Œæ¯•ï¼Œå‰©ä½™ {len(valid_data)} æ¡ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16a39532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒæ•°æ®åˆ†è¯å®Œæ¯•ã€‚\n",
      "éªŒè¯æ•°æ®åˆ†è¯å®Œæ¯•ã€‚\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = \"bert-base-multilingual-cased\" # å¤šè¯­è¨€BERTåˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_data(data, tokenizer, max_length=128):\n",
    "    source_texts = [item['english'] for item in data]\n",
    "    target_texts = [item['chinese'] for item in data]\n",
    "\n",
    "    tokenized_data = tokenizer(source_texts, text_target=target_texts, \n",
    "                              max_length=max_length, truncation=True, padding='max_length')\n",
    "    return tokenized_data\n",
    "\n",
    "train_data = tokenize_data(train_data, tokenizer)\n",
    "valid_data = tokenize_data(valid_data, tokenizer)\n",
    "\n",
    "print(\"è®­ç»ƒæ•°æ®åˆ†è¯å®Œæ¯•ã€‚\")\n",
    "print(\"éªŒè¯æ•°æ®åˆ†è¯å®Œæ¯•ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be9792be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæ¯•ã€‚\n",
      "éªŒè¯æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæ¯•ã€‚\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data)\n",
    "valid_dataset = TranslationDataset(valid_data)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=8)\n",
    "\n",
    "print(\"è®­ç»ƒæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæ¯•ã€‚\")\n",
    "print(\"éªŒè¯æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæ¯•ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 78,238,208 || trainable%: 0.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\training_args.py:1577: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mbleu\u001b[39m\u001b[33m\"\u001b[39m: bleu_result[\u001b[33m\"\u001b[39m\u001b[33mbleu\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# åˆ›å»ºTrainerå®ä¾‹å¹¶è¿›è¡Œè®­ç»ƒ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\trainer.py:465\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28mself\u001b[39m.compute_loss_func = compute_loss_func\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m enable_full_determinism(\u001b[38;5;28mself\u001b[39m.args.seed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.full_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28mself\u001b[39m.deepspeed = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\trainer_utils.py:106\u001b[39m, in \u001b[36mset_seed\u001b[39m\u001b[34m(seed, deterministic)\u001b[39m\n\u001b[32m    104\u001b[39m np.random.seed(seed)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     torch.cuda.manual_seed_all(seed)\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\random.py:46\u001b[39m, in \u001b[36mmanual_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda._is_in_bad_fork():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.mps._is_in_bad_fork():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\cuda\\random.py:127\u001b[39m, in \u001b[36mmanual_seed_all\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m    124\u001b[39m         default_generator = torch.cuda.default_generators[i]\n\u001b[32m    125\u001b[39m         default_generator.manual_seed(seed)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:249\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, **kwargs):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[32m    253\u001b[39m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\cuda\\random.py:125\u001b[39m, in \u001b[36mmanual_seed_all.<locals>.cb\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[32m    124\u001b[39m     default_generator = torch.cuda.default_generators[i]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     default_generator.manual_seed(seed)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "\n",
    "# é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\" # ç¤ºä¾‹ï¼šä¸€ä¸ªå°å‹NMTæ¨¡å‹\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# é…ç½®LoRAï¼ˆå¦‚æœä½¿ç”¨PEFTï¼‰\n",
    "peft_config = LoraConfig(\n",
    "    r=8, # LoRA ç§©\n",
    "    lora_alpha=16, # LoRA ç¼©æ”¾å› å­\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, # åºåˆ—åˆ°åºåˆ—è¯­è¨€æ¨¡å‹ä»»åŠ¡\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # ç›®æ ‡æ¨¡å—ï¼Œé€šå¸¸æ˜¯æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢å’Œå€¼æŠ•å½±å±‚\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters() # æ‰“å°å¯è®­ç»ƒå‚æ•°\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "# å®šä¹‰è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•°\n",
    "from evaluate import load\n",
    "bleu_metric = load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [[label] for label in decoded_labels] # è°ƒæ•´referencesæ ¼å¼\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_result[\"bleu\"]}\n",
    "\n",
    "# åˆ›å»ºTrainerå®ä¾‹å¹¶è¿›è¡Œè®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_length=128):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚\n",
    "\n",
    "    Args:\n",
    "        model: è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚\n",
    "        tokenizer: åˆ†è¯å™¨ã€‚\n",
    "        test_dataset: æµ‹è¯•æ•°æ®é›†ã€‚\n",
    "        max_length (int, optional): æœ€å¤§ç”Ÿæˆé•¿åº¦ã€‚\n",
    "\n",
    "    Returns:\n",
    "        dict: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # ç”Ÿæˆç¿»è¯‘ç»“æœ\n",
    "    predictions = []\n",
    "    references = []\n",
    "    model.eval() # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    for batch in test_dataset:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].unsqueeze(0).to(model.device)\n",
    "            generated_ids = model.generate(input_ids, max_length=max_length)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            predictions.extend(decoded_preds)\n",
    "\n",
    "        labels = batch['labels'].numpy()\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # Replace masked label ids\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        references.extend(decoded_labels)\n",
    "\n",
    "    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "    bleu = load('bleu')\n",
    "    chrf = load('chrf')\n",
    "    comet = load('comet') # éœ€è¦å®‰è£… 'unbabel-comet'\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])['bleu']\n",
    "    chrf_score = chrf.compute(predictions=predictions, references=[[ref] for ref in references])['score']\n",
    "    comet_score = comet.compute(predictions=predictions, references=[[ref] for ref in references], sources=[[src] for src in source_texts])['mean_score']\n",
    "\n",
    "    return {'bleu': bleu_score, 'chrf': chrf_score, 'comet': comet_score}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
