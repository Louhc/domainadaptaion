{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2960ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f913272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Number of training samples: 10000\n",
      "Number of validation samples: 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path, max_lines=float(\"inf\")):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        n = 0\n",
    "        for line in f:\n",
    "            n += 1\n",
    "            if n > max_lines: break\n",
    "\n",
    "            try:\n",
    "                item = json.loads(line.strip())\n",
    "                data.append(item)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解码错误：{e}，跳过该行\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "train_data = load_data('translation2019zh_train.json', 10000)\n",
    "valid_data = load_data('translation2019zh_valid.json', 100)\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a65693c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据清洗完毕，剩余 10000 条。\n",
      "验证数据清洗完毕，剩余 100 条。\n"
     ]
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    seen = set()\n",
    "    for item in data:\n",
    "        en = item['english'].strip()\n",
    "        zh = item['chinese'].strip()\n",
    "        if (en, zh) not in seen:\n",
    "            cleaned_data.append({'english': en, 'chinese': zh})\n",
    "            seen.add((en, zh))\n",
    "    return cleaned_data\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "valid_data = clean_data(valid_data)\n",
    "\n",
    "print(f\"训练数据清洗完毕，剩余 {len(train_data)} 条。\")\n",
    "print(f\"验证数据清洗完毕，剩余 {len(valid_data)} 条。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16a39532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据分词完毕。\n",
      "验证数据分词完毕。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = \"bert-base-multilingual-cased\" # 多语言BERT分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_data(data, tokenizer, max_length=128):\n",
    "    source_texts = [item['english'] for item in data]\n",
    "    target_texts = [item['chinese'] for item in data]\n",
    "\n",
    "    tokenized_data = tokenizer(source_texts, text_target=target_texts, \n",
    "                              max_length=max_length, truncation=True, padding='max_length')\n",
    "    return tokenized_data\n",
    "\n",
    "train_data = tokenize_data(train_data, tokenizer)\n",
    "valid_data = tokenize_data(valid_data, tokenizer)\n",
    "\n",
    "print(\"训练数据分词完毕。\")\n",
    "print(\"验证数据分词完毕。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be9792be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据加载器创建完毕。\n",
      "验证数据加载器创建完毕。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data)\n",
    "valid_dataset = TranslationDataset(valid_data)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=8)\n",
    "\n",
    "print(\"训练数据加载器创建完毕。\")\n",
    "print(\"验证数据加载器创建完毕。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 78,238,208 || trainable%: 0.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\training_args.py:1577: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mbleu\u001b[39m\u001b[33m\"\u001b[39m: bleu_result[\u001b[33m\"\u001b[39m\u001b[33mbleu\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 创建Trainer实例并进行训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\trainer.py:465\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28mself\u001b[39m.compute_loss_func = compute_loss_func\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m enable_full_determinism(\u001b[38;5;28mself\u001b[39m.args.seed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.full_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28mself\u001b[39m.deepspeed = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\transformers\\trainer_utils.py:106\u001b[39m, in \u001b[36mset_seed\u001b[39m\u001b[34m(seed, deterministic)\u001b[39m\n\u001b[32m    104\u001b[39m np.random.seed(seed)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     torch.cuda.manual_seed_all(seed)\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\random.py:46\u001b[39m, in \u001b[36mmanual_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda._is_in_bad_fork():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.mps._is_in_bad_fork():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\cuda\\random.py:127\u001b[39m, in \u001b[36mmanual_seed_all\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m    124\u001b[39m         default_generator = torch.cuda.default_generators[i]\n\u001b[32m    125\u001b[39m         default_generator.manual_seed(seed)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:249\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, **kwargs):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[32m    253\u001b[39m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\domainadaptaion\\venv\\Lib\\site-packages\\torch\\cuda\\random.py:125\u001b[39m, in \u001b[36mmanual_seed_all.<locals>.cb\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[32m    124\u001b[39m     default_generator = torch.cuda.default_generators[i]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     default_generator.manual_seed(seed)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "\n",
    "# 选择一个合适的预训练模型\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\" # 示例：一个小型NMT模型\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 配置LoRA（如果使用PEFT）\n",
    "peft_config = LoraConfig(\n",
    "    r=8, # LoRA 秩\n",
    "    lora_alpha=16, # LoRA 缩放因子\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, # 序列到序列语言模型任务\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # 目标模块，通常是注意力机制中的查询和值投影层\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters() # 打印可训练参数\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "# 定义评估指标计算函数\n",
    "from evaluate import load\n",
    "bleu_metric = load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [[label] for label in decoded_labels] # 调整references格式\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_result[\"bleu\"]}\n",
    "\n",
    "# 创建Trainer实例并进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_dataset, max_length=128):\n",
    "    \"\"\"\n",
    "    评估模型性能。\n",
    "\n",
    "    Args:\n",
    "        model: 训练好的模型。\n",
    "        tokenizer: 分词器。\n",
    "        test_dataset: 测试数据集。\n",
    "        max_length (int, optional): 最大生成长度。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含评估指标的字典。\n",
    "    \"\"\"\n",
    "\n",
    "    # 生成翻译结果\n",
    "    predictions = []\n",
    "    references = []\n",
    "    model.eval() # 设置模型为评估模式\n",
    "    for batch in test_dataset:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].unsqueeze(0).to(model.device)\n",
    "            generated_ids = model.generate(input_ids, max_length=max_length)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            predictions.extend(decoded_preds)\n",
    "\n",
    "        labels = batch['labels'].numpy()\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # Replace masked label ids\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        references.extend(decoded_labels)\n",
    "\n",
    "    # 计算评估指标\n",
    "    bleu = load('bleu')\n",
    "    chrf = load('chrf')\n",
    "    comet = load('comet') # 需要安装 'unbabel-comet'\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])['bleu']\n",
    "    chrf_score = chrf.compute(predictions=predictions, references=[[ref] for ref in references])['score']\n",
    "    comet_score = comet.compute(predictions=predictions, references=[[ref] for ref in references], sources=[[src] for src in source_texts])['mean_score']\n",
    "\n",
    "    return {'bleu': bleu_score, 'chrf': chrf_score, 'comet': comet_score}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
